{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9ZssAPDW2Rz"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')\n",
    "# %cd /gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OT5vot-kbFVA"
   },
   "outputs": [],
   "source": [
    "# # 指定当前的工作文件夹\n",
    "# import os\n",
    "\n",
    "# # 此处为google drive中的文件路径,drive为之前指定的工作根目录，要加上\n",
    "# os.chdir(\"My Drive/nmt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNqP0B5TGkPh"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import jieba\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDV7QiADc15p"
   },
   "outputs": [],
   "source": [
    "# padding\n",
    "def padding(data, max_len):\n",
    "    # 对长度小于max的以0填充，大于的截断，\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(data, max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WAGxDajuGlCH"
   },
   "outputs": [],
   "source": [
    "# eval_size = 0.1\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "lr = 1e-3\n",
    "optimizer = 'adam'\n",
    "dropout = 0.2\n",
    "\n",
    "# 60004\n",
    "# 7027\n",
    "src_max_vocab_size = 60004\n",
    "tgt_max_vocab_size = 7027\n",
    "embedding_size = 128\n",
    "hidden_size = 256\n",
    "src_max_seq_len = 40    # 源句的裁剪长度\n",
    "tgt_max_seq_len = 40       # 目标句的裁剪长度\n",
    "tgt_start_id = 2 # <S> \n",
    "tgt_end_id = 0 # <PAD>\n",
    "max_gradient_norm = 1.\n",
    "maximum_iterations = 40  #!!!    目标句 测试或验证 的翻译长度\n",
    "cf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "cf.gpu_options.allow_growth = True\n",
    "cf.gpu_options.per_process_gpu_memory_fraction = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEnsHFOwGo9O"
   },
   "outputs": [],
   "source": [
    "class Iterator(object):\n",
    "    \"\"\"\n",
    "    数据迭代器\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):  # train_en_corpus, train_ch_corpus\n",
    "        self.x = np.asarray(x)\n",
    "        self.y = np.asarray(y)\n",
    "        self.sample_num = self.x.shape[0]\n",
    "\n",
    "    def next_batch(self, batch_size): # 用于train\n",
    "        # produce X, Y_out, Y_in, X_len, Y_in_len, Y_out_len\n",
    "        l = np.random.randint(0, self.sample_num - batch_size + 1)\n",
    "        r = l + batch_size\n",
    "        x_part = self.x[l:r]\n",
    "        y_out_part = self.y[l:r]\n",
    "        x_len = np.sum((x_part > 0), axis=1)\n",
    "        y_in_part = np.concatenate((np.ones((batch_size, 1)) * 2, y_out_part[:,:-1]), axis=-1)\n",
    "        max_y_dim = self.y.shape[1]\n",
    "        y_out_len = np.sum((y_out_part > 0), axis=1) + 1\n",
    "        y_out_len = np.min(np.concatenate([np.ones((batch_size, 1)) * max_y_dim, y_out_len.reshape(-1, 1)], axis=-1), axis=-1)\n",
    "        y_in_len = np.sum((y_in_part > 0), axis=1) + 1\n",
    "        y_in_len = np.min(np.concatenate([np.ones((batch_size, 1)) * max_y_dim, y_in_len.reshape(-1, 1)], axis=-1), axis=-1)\n",
    "        x_len = x_len.astype(np.int32)\n",
    "        y_in_part = y_in_part.astype(np.int32)\n",
    "        y_in_len = y_in_len.astype(np.int32)\n",
    "        y_out_len = y_out_len.astype(np.int32)\n",
    "        return x_part, y_out_part, y_in_part, x_len, y_in_len, y_out_len\n",
    "        # x_part 中文某batch_size句话； y_out_part：对应英文某batch_size句话； x_len：所有句子去掉padding的总长度；\n",
    "\n",
    "    def next(self, batch_size): #用于evaluate\n",
    "        l = 0\n",
    "        while l < self.sample_num:\n",
    "            r = min(l + batch_size, self.sample_num)\n",
    "            batch_size = r - l\n",
    "            x_part = self.x[l:r] # 没有要第一句\n",
    "            y_out_part = self.y[l:r]\n",
    "            x_len = np.sum((x_part > 0), axis=1) # 去掉填充的0之后，每条句子的长度\n",
    "            y_in_part = np.concatenate((np.ones((batch_size, 1)) * 2, y_out_part[:,:-1]), axis=-1) # 在第一列加了一列 2\n",
    "            max_y_dim = self.y.shape[1] # 句子长度\n",
    "            \n",
    "            y_out_len = np.sum((y_out_part > 0), axis=1) + 1\n",
    "            # 每条句子返回max_y_dim 与 y_out_part句子长度的较小值\n",
    "            y_out_len = np.min(np.concatenate([np.ones((batch_size, 1)) * max_y_dim, y_out_len.reshape(-1, 1)], axis=-1), axis=-1) # reshape(-1, 1) 表示行数未知，列数为1\n",
    "            y_in_len = np.sum((y_in_part > 0), axis=1) + 1\n",
    "            y_in_len = np.min(np.concatenate([np.ones((batch_size, 1)) * max_y_dim, y_in_len.reshape(-1, 1)], axis=-1), axis=-1)\n",
    "            \n",
    "            x_len = x_len.astype(np.int32)\n",
    "            y_in_part = y_in_part.astype(np.int32)\n",
    "            y_in_len = y_in_len.astype(np.int32)\n",
    "            y_out_len = y_out_len.astype(np.int32)\n",
    "            l += batch_size\n",
    "            yield x_part, y_out_part, y_in_part, x_len, y_in_len, y_out_len\n",
    "            \n",
    "            \n",
    "            \n",
    "class NMTModel(object):\n",
    "    \"\"\"\n",
    "    带Attention的NMT模型\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 src_max_vocab_size, \n",
    "                 tgt_max_vocab_size, \n",
    "                 embedding_size,\n",
    "                 hidden_size,\n",
    "                 src_max_seq_len,\n",
    "                 tgt_max_seq_len,\n",
    "                 tgt_start_id,\n",
    "                 tgt_end_id,\n",
    "                 max_gradient_norm=5,\n",
    "                 maximum_iterations=None,\n",
    "                 optimizer='adam',\n",
    "                 ):\n",
    "        self.initializer = tf.random_uniform_initializer(\n",
    "        -0.05, 0.05)\n",
    "        self.optimizer = optimizer\n",
    "        # 源词表大小\n",
    "        self.src_max_vocab_size = src_max_vocab_size\n",
    "        # 目标词表大小\n",
    "        self.tgt_max_vocab_size = tgt_max_vocab_size\n",
    "        # 输入embedding大小（src与tgt的embedding_size可以不同）\n",
    "        self.embedding_size = embedding_size\n",
    "        # 隐层大小\n",
    "        self.hidden_size = hidden_size\n",
    "        # 源序列长度\n",
    "        self.src_max_seq_len = src_max_seq_len\n",
    "        # 目标序列长度\n",
    "        self.tgt_max_seq_len = tgt_max_seq_len\n",
    "        # 目标序列起始id（输入的初始id值）\n",
    "        self.tgt_start_id = tgt_start_id\n",
    "        # 目标的终结id（模型预测到该id后停止预测）\n",
    "        self.tgt_end_id = tgt_end_id\n",
    "        if maximum_iterations is None:\n",
    "            self.maximum_iterations = self.tgt_max_seq_len                                  # !!!!!!!!!!!\n",
    "        else:\n",
    "            self.maximum_iterations = maximum_iterations\n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "        self.add_placeholders()\n",
    "        self.batch_size = tf.shape(self.X)[0]\n",
    "        self.add_embeddings()\n",
    "        self.encoder()\n",
    "        self.decoder()\n",
    "        self.add_loss()\n",
    "        self.add_train_op()\n",
    "\n",
    "    def add_placeholders(self):  # 占位，不定长\n",
    "        # X, Y_out, Y_in, X_len, Y_in_len, Y_out_len\n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y_out = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y_in = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_len = tf.placeholder(tf.int32, [None, ])\n",
    "        self.Y_in_len = tf.placeholder(tf.int32, [None, ])\n",
    "        self.Y_out_len = tf.placeholder(tf.int32, [None, ])\n",
    "        self.lr = tf.placeholder(tf.float32)\n",
    "        self.dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "    def add_embeddings(self):\n",
    "        with tf.variable_scope('embeddings', initializer=self.initializer):\n",
    "            # 创建变量\n",
    "            self.X_emb = tf.get_variable('X_emb', \n",
    "                shape=(self.src_max_vocab_size, self.embedding_size), \n",
    "                dtype=tf.float32)\n",
    "            self.Y_emb = tf.get_variable('Y_emb', \n",
    "                shape=(self.tgt_max_vocab_size, self.embedding_size), \n",
    "                dtype=tf.float32)\n",
    "\n",
    "            self.encoder_input = tf.nn.embedding_lookup(self.X_emb, self.X)\n",
    "            self.decoder_input = tf.nn.embedding_lookup(self.Y_emb, self.Y_in)\n",
    "\n",
    "    def encoder(self):\n",
    "        with tf.variable_scope('encoder'):\n",
    "            fw_encoder_cell = tf.contrib.rnn.GRUCell(self.hidden_size)\n",
    "            fw_encoder_cell = tf.contrib.rnn.DropoutWrapper(fw_encoder_cell, input_keep_prob=1-self.dropout)\n",
    "            bw_encoder_cell = tf.contrib.rnn.GRUCell(self.hidden_size)\n",
    "            bw_encoder_cell = tf.contrib.rnn.DropoutWrapper(bw_encoder_cell, input_keep_prob=1-self.dropout)\n",
    "\n",
    "            # 双向RNN\n",
    "            encoder_outputs, bi_last_state = tf.nn.bidirectional_dynamic_rnn(     \n",
    "                    fw_encoder_cell, bw_encoder_cell, self.encoder_input, \n",
    "                    self.X_len, dtype=tf.float32)\n",
    "            self.encoder_outputs = tf.concat(encoder_outputs, axis=-1)\n",
    "            self.encoder_last_state = bi_last_state\n",
    "\n",
    "\n",
    "    def decoder(self):\n",
    "        with tf.variable_scope('decoder'):\n",
    "            decoder_cell = tf.contrib.rnn.GRUCell(self.hidden_size)\n",
    "            decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=1-self.dropout)\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                                    self.hidden_size, self.encoder_outputs,\n",
    "                                    memory_sequence_length=self.X_len)\n",
    "            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                                    decoder_cell, attention_mechanism,\n",
    "                                    attention_layer_size=self.hidden_size)\n",
    "\n",
    "            projection_layer = layers_core.Dense(\n",
    "            self.tgt_max_vocab_size, u+se_bias=False)\n",
    "\n",
    "        # 训练或评估的时候，decoder的output是真实的target，input是target右移一个词\n",
    "        with tf.variable_scope('dynamic_decode'):\n",
    "            # Helper\n",
    "            helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                self.decoder_input, tf.ones((self.batch_size, ), dtype=tf.int32) * self.tgt_max_seq_len, time_major=False)\n",
    "            # Decoder\n",
    "            decoder_initial_state = decoder_cell.zero_state(self.batch_size, dtype=tf.float32).clone(\n",
    "                cell_state=self.encoder_last_state[0])\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                decoder_cell, helper, decoder_initial_state,\n",
    "                output_layer=projection_layer)\n",
    "            # Dynamic decoding\n",
    "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "            self.logits = outputs.rnn_output\n",
    "            self.pred = tf.argmax(self.logits, axis=2)\n",
    "\n",
    "        # 预测的时候，decoder的每个timestep的输入为前一个时刻的输出\n",
    "        with tf.variable_scope('dynamic_decode', reuse=True):\n",
    "            # Helper\n",
    "            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                self.Y_emb,\n",
    "                start_tokens=tf.fill([self.batch_size], self.tgt_start_id),\n",
    "                end_token=self.tgt_end_id)\n",
    "            decoder_initial_state = decoder_cell.zero_state(self.batch_size, dtype=tf.float32).clone(\n",
    "                cell_state=self.encoder_last_state[0])\n",
    "            # Decoder\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                decoder_cell, helper, decoder_initial_state,\n",
    "                output_layer=projection_layer)\n",
    "            # Dynamic decoding\n",
    "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder, maximum_iterations=self.maximum_iterations)                      # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            self.translations = outputs.sample_id\n",
    "\n",
    "    def add_loss(self): # 交叉熵\n",
    "        target_weights = tf.sequence_mask(\n",
    "                         self.Y_out_len, self.tgt_max_seq_len, dtype=self.logits.dtype)\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                         labels=self.Y_out, logits=self.logits)\n",
    "        self.loss_op = (tf.reduce_sum(crossent * target_weights) / tf.to_float(self.batch_size))\n",
    "\n",
    "    def add_train_op(self):   # 优化器\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(self.loss_op, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "            gradients, self.max_gradient_norm)\n",
    "        # Optimization\n",
    "        if self.optimizer == 'sgd':\n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        elif self.optimizer == 'adadelta':\n",
    "            optimizer = tf.train.AdaDeltaOptimizer(self.lr)\n",
    "        else:\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(clipped_gradients, params), global_step=self.global_step)\n",
    "        \n",
    "        \n",
    "def transform2word(data, id2word):\n",
    "    \"\"\"\n",
    "    把id转成word\n",
    "    \"\"\"\n",
    "    ret_data = []\n",
    "    for sentence in data:\n",
    "        ret_data.append(''.join([id2word.get(word, '<UNK>') for word in sentence]))\n",
    "    return ret_data\n",
    "\n",
    "def bleu(refs, hyps):\n",
    "    \"\"\"\n",
    "    计算bleu-4\n",
    "    \"\"\"\n",
    "    refs = [[[_ for _ in ref if _ > 0]] for ref in refs]\n",
    "    hyps = [[_ for _ in hyp if _ > 0] for hyp in hyps]\n",
    "    return nltk.translate.bleu_score.corpus_bleu(refs, hyps)\n",
    "\n",
    "def train():\n",
    "    # load data and dictionary\n",
    "    with open(r'data\\vocab_dict_and_corpus_biEduNew.pkl', 'rb') as fr:\n",
    "        en_word2id, en_id2word, ch_word2id, ch_id2word, \\\n",
    "        train_en_corpus, train_ch_corpus, eval_en_corpus, eval_ch_corpus = pkl.load(fr)\n",
    "    \n",
    "    train_en_corpus = padding(train_en_corpus, src_max_seq_len)\n",
    "    eval_en_corpus = padding(eval_en_corpus, src_max_seq_len)\n",
    "    train_ch_corpus = padding(train_ch_corpus, tgt_max_seq_len)\n",
    "    eval_ch_corpus = padding(eval_ch_corpus, tgt_max_seq_len)\n",
    "    \n",
    "    # truncate the vocabrary\n",
    "    # for the words exceeded te vocab size, we set it as 1(<UNK>)\n",
    "    train_en_corpus[train_en_corpus >= src_max_vocab_size] = 1  # 字典中的单词是按频率高低排序的，因此这一步的目的是不考虑频率过低的单词\n",
    "    eval_en_corpus[eval_en_corpus >= src_max_vocab_size] = 1\n",
    "    train_ch_corpus[train_ch_corpus >= tgt_max_vocab_size] = 1\n",
    "    eval_ch_corpus[eval_ch_corpus >= tgt_max_vocab_size] = 1\n",
    "    \n",
    "#     train_en_corpus, eval_en_corpus, train_ch_corpus, eval_ch_corpus = train_test_split(train_en_corpus, train_ch_corpus, test_size=0.2, )\n",
    "\n",
    "    print('train size:{}, val size:{}'.format(train_en_corpus.shape, eval_en_corpus.shape))\n",
    "\n",
    "    iter_num = train_en_corpus.shape[0] // batch_size + 1  # 取整\n",
    "#     iter_num = 10\n",
    "\n",
    "    data_iterator = Iterator(train_en_corpus, train_ch_corpus)\n",
    "    eval_data_iterator = Iterator(eval_en_corpus, eval_ch_corpus)\n",
    "    now_lr = lr\n",
    "    with tf.Session(config=cf) as sess:\n",
    "        model = NMTModel(src_max_vocab_size=src_max_vocab_size, \n",
    "                         tgt_max_vocab_size=tgt_max_vocab_size, \n",
    "                         embedding_size=embedding_size,\n",
    "                         hidden_size=hidden_size,\n",
    "                         src_max_seq_len=src_max_seq_len,\n",
    "                         tgt_max_seq_len=tgt_max_seq_len,\n",
    "                         tgt_start_id=tgt_start_id,\n",
    "                         tgt_end_id=tgt_end_id,\n",
    "                         max_gradient_norm=max_gradient_norm,\n",
    "                         maximum_iterations=maximum_iterations,\n",
    "                         optimizer=optimizer)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        max_bleu = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"\\nepoch \", epoch, \"\\n\")\n",
    "            for iter_n in range(iter_num):\n",
    "                X, Y_out, Y_in, X_len, Y_in_len, Y_out_len = data_iterator.next_batch(batch_size)\n",
    "\n",
    "                loss, _, global_step = sess.run([model.loss_op, model.train_op, model.global_step], \n",
    "                    feed_dict={ model.X:X,\n",
    "                                model.Y_out:Y_out,\n",
    "                                model.Y_in:Y_in, \n",
    "                                model.X_len:X_len,\n",
    "                                model.Y_in_len:Y_in_len,\n",
    "                                model.Y_out_len:Y_out_len,\n",
    "                                model.lr:now_lr,\n",
    "                                model.dropout:dropout})\n",
    "                if iter_n % 100 == 0:\n",
    "                    print('iter:{}, train loss:{}'.format(iter_n, loss))\n",
    "            if optimizer == 'sgd':\n",
    "                now_lr = now_lr / 2\n",
    "            bleu = evaluate(model, sess, eval_data_iterator)\n",
    "            if bleu>max_bleu:\n",
    "                max_bleu = bleu\n",
    "                saver.save(sess,'model/my_model', global_step=global_step)\n",
    "            print(\"max_bleu = \", max_bleu)\n",
    "\n",
    "\n",
    "def evaluate(model, sess, data_iterator):\n",
    "    translations = []\n",
    "    refs = []\n",
    "    losses = []\n",
    "    for X, Y_out, Y_in, X_len, Y_in_len, Y_out_len in data_iterator.next(batch_size):\n",
    "        loss, translation = sess.run([model.loss_op, model.translations], \n",
    "                        feed_dict={ model.X:X,\n",
    "                                    model.Y_in:Y_in,\n",
    "                                    model.Y_out:Y_out,\n",
    "                                    model.X_len:X_len,\n",
    "                                    model.Y_in_len:Y_in_len,\n",
    "                                    model.Y_out_len:Y_out_len,\n",
    "                                    model.lr:lr,\n",
    "                                    model.dropout:0.})\n",
    "\n",
    "#         print('translation.shape:',translation.shape)\n",
    "        if translation.shape[1]<maximum_iterations:\n",
    "            translation = np.pad(translation,((0,0),(0,maximum_iterations-translation.shape[1])),'constant')\n",
    "        translations.append(translation)\n",
    "        refs.append(Y_out)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    translations = np.concatenate(translations, axis=0)\n",
    "    refs = np.concatenate(refs, axis=0)\n",
    "    bleu_score = bleu(refs, translations)\n",
    "    print('bleu score:{}, loss:{}'.format(bleu_score, np.mean(loss)))\n",
    "    return bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5G8LIHqGsw6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:(445000, 40), val size:(2500, 40)\n",
      "WARNING:tensorflow:From C:\\Users\\19843\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-13-e953f40fb2e6>:130: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-13-e953f40fb2e6>:138: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\19843\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\19843\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\19843\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-13-e953f40fb2e6>:196: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "epoch  0 \n",
      "\n",
      "iter:0, train loss:228.6341552734375\n",
      "iter:100, train loss:222.26649475097656\n",
      "iter:200, train loss:235.4803924560547\n",
      "iter:300, train loss:172.89389038085938\n",
      "iter:400, train loss:160.52182006835938\n",
      "iter:500, train loss:176.88668823242188\n",
      "iter:600, train loss:139.56365966796875\n",
      "iter:700, train loss:146.4503936767578\n",
      "iter:800, train loss:193.30381774902344\n",
      "iter:900, train loss:135.66665649414062\n",
      "iter:1000, train loss:169.15673828125\n",
      "iter:1100, train loss:202.48768615722656\n",
      "iter:1200, train loss:173.85621643066406\n",
      "iter:1300, train loss:182.5785369873047\n",
      "iter:1400, train loss:191.2032470703125\n",
      "iter:1500, train loss:139.38552856445312\n",
      "iter:1600, train loss:157.22683715820312\n",
      "iter:1700, train loss:161.68345642089844\n",
      "iter:1800, train loss:163.4115447998047\n",
      "iter:1900, train loss:154.8525848388672\n",
      "iter:2000, train loss:166.5682373046875\n",
      "iter:2100, train loss:157.56114196777344\n",
      "iter:2200, train loss:158.0056915283203\n",
      "iter:2300, train loss:150.5234832763672\n",
      "iter:2400, train loss:163.9021453857422\n",
      "iter:2500, train loss:119.1192398071289\n",
      "iter:2600, train loss:154.33151245117188\n",
      "iter:2700, train loss:113.0372314453125\n",
      "iter:2800, train loss:164.8880157470703\n",
      "iter:2900, train loss:162.80186462402344\n",
      "iter:3000, train loss:133.59019470214844\n",
      "iter:3100, train loss:156.1836395263672\n",
      "iter:3200, train loss:114.04733276367188\n",
      "iter:3300, train loss:108.98169708251953\n",
      "iter:3400, train loss:143.4999237060547\n",
      "iter:3500, train loss:107.36808776855469\n",
      "iter:3600, train loss:143.94589233398438\n",
      "iter:3700, train loss:111.51231384277344\n",
      "iter:3800, train loss:151.77243041992188\n",
      "iter:3900, train loss:131.5992431640625\n",
      "iter:4000, train loss:134.03053283691406\n",
      "iter:4100, train loss:152.74859619140625\n",
      "iter:4200, train loss:113.92803955078125\n",
      "iter:4300, train loss:108.64984893798828\n",
      "iter:4400, train loss:131.23780822753906\n",
      "iter:4500, train loss:153.12449645996094\n",
      "iter:4600, train loss:122.96803283691406\n",
      "iter:4700, train loss:95.5548324584961\n",
      "iter:4800, train loss:109.57770538330078\n",
      "iter:4900, train loss:153.88975524902344\n",
      "iter:5000, train loss:110.87117004394531\n",
      "iter:5100, train loss:148.15431213378906\n",
      "iter:5200, train loss:141.0394744873047\n",
      "iter:5300, train loss:145.0181121826172\n",
      "iter:5400, train loss:149.68539428710938\n",
      "iter:5500, train loss:95.73226928710938\n",
      "iter:5600, train loss:109.91124725341797\n",
      "iter:5700, train loss:133.74472045898438\n",
      "iter:5800, train loss:109.89530181884766\n",
      "iter:5900, train loss:102.69596862792969\n",
      "iter:6000, train loss:126.66822814941406\n",
      "iter:6100, train loss:96.91355895996094\n",
      "iter:6200, train loss:136.03038024902344\n",
      "iter:6300, train loss:100.1755142211914\n",
      "iter:6400, train loss:89.8382339477539\n",
      "iter:6500, train loss:129.20741271972656\n",
      "iter:6600, train loss:100.73895263671875\n",
      "iter:6700, train loss:108.69155883789062\n",
      "iter:6800, train loss:105.63970947265625\n",
      "iter:6900, train loss:93.1793212890625\n",
      "bleu score:0.07773209954808745, loss:131.19729614257812\n",
      "max_bleu =  0.07773209954808745\n",
      "\n",
      "epoch  1 \n",
      "\n",
      "iter:0, train loss:97.05073547363281\n",
      "iter:100, train loss:122.70092010498047\n",
      "iter:200, train loss:125.2387466430664\n",
      "iter:300, train loss:104.5650634765625\n",
      "iter:400, train loss:114.82121276855469\n",
      "iter:500, train loss:95.53956604003906\n",
      "iter:600, train loss:142.712646484375\n",
      "iter:700, train loss:115.60127258300781\n",
      "iter:800, train loss:92.71648406982422\n",
      "iter:900, train loss:93.91374206542969\n",
      "iter:1000, train loss:105.2498550415039\n",
      "iter:1100, train loss:135.55194091796875\n",
      "iter:1200, train loss:118.53156280517578\n",
      "iter:1300, train loss:98.04026794433594\n",
      "iter:1400, train loss:126.93083190917969\n",
      "iter:1500, train loss:128.78440856933594\n",
      "iter:1600, train loss:103.14925384521484\n",
      "iter:1700, train loss:133.11968994140625\n",
      "iter:1800, train loss:101.39349365234375\n",
      "iter:1900, train loss:96.04280853271484\n",
      "iter:2000, train loss:105.8821029663086\n",
      "iter:2100, train loss:110.3127670288086\n",
      "iter:2200, train loss:123.56914520263672\n",
      "iter:2300, train loss:134.19898986816406\n",
      "iter:2400, train loss:110.70445251464844\n",
      "iter:2500, train loss:73.79206085205078\n",
      "iter:2600, train loss:114.7656021118164\n",
      "iter:2700, train loss:124.88703155517578\n",
      "iter:2800, train loss:89.23760986328125\n",
      "iter:2900, train loss:119.95805358886719\n",
      "iter:3000, train loss:132.43646240234375\n",
      "iter:3100, train loss:123.44564819335938\n",
      "iter:3200, train loss:73.2154312133789\n",
      "iter:3300, train loss:91.10345458984375\n",
      "iter:3400, train loss:126.66195678710938\n",
      "iter:3500, train loss:120.93597412109375\n",
      "iter:3600, train loss:97.6111831665039\n",
      "iter:3700, train loss:90.1060562133789\n",
      "iter:3800, train loss:94.78514099121094\n",
      "iter:3900, train loss:102.82978820800781\n",
      "iter:4000, train loss:101.5157470703125\n",
      "iter:4100, train loss:87.44181060791016\n",
      "iter:4200, train loss:82.35576629638672\n",
      "iter:4300, train loss:110.79338073730469\n",
      "iter:4400, train loss:117.23464965820312\n",
      "iter:4500, train loss:80.5267562866211\n",
      "iter:4600, train loss:81.1885986328125\n",
      "iter:4700, train loss:117.23870849609375\n",
      "iter:4800, train loss:95.23192596435547\n",
      "iter:4900, train loss:120.79252624511719\n",
      "iter:5000, train loss:93.84766387939453\n",
      "iter:5100, train loss:99.34526062011719\n",
      "iter:5200, train loss:93.64192199707031\n",
      "iter:5300, train loss:134.52822875976562\n",
      "iter:5400, train loss:116.09562683105469\n",
      "iter:5500, train loss:93.78861999511719\n",
      "iter:5600, train loss:85.108642578125\n",
      "iter:5700, train loss:114.61890411376953\n",
      "iter:5800, train loss:86.70984649658203\n",
      "iter:5900, train loss:91.90724182128906\n",
      "iter:6000, train loss:98.29325866699219\n",
      "iter:6100, train loss:99.40383911132812\n",
      "iter:6200, train loss:104.37879943847656\n",
      "iter:6300, train loss:100.93879699707031\n",
      "iter:6400, train loss:78.92923736572266\n",
      "iter:6500, train loss:117.52750396728516\n",
      "iter:6600, train loss:72.75003814697266\n",
      "iter:6700, train loss:81.50995635986328\n",
      "iter:6800, train loss:125.22616577148438\n",
      "iter:6900, train loss:106.46009063720703\n",
      "bleu score:0.11402621902842419, loss:119.25359344482422\n",
      "max_bleu =  0.11402621902842419\n",
      "\n",
      "epoch  2 \n",
      "\n",
      "iter:0, train loss:83.53076171875\n",
      "iter:100, train loss:88.25935363769531\n",
      "iter:200, train loss:99.9693603515625\n",
      "iter:300, train loss:95.96890258789062\n",
      "iter:400, train loss:118.69234466552734\n",
      "iter:500, train loss:80.53657531738281\n",
      "iter:600, train loss:89.88409423828125\n",
      "iter:700, train loss:116.11023712158203\n",
      "iter:800, train loss:73.51564025878906\n",
      "iter:900, train loss:97.06775665283203\n",
      "iter:1000, train loss:90.08311462402344\n",
      "iter:1100, train loss:103.0643081665039\n",
      "iter:1200, train loss:75.39507293701172\n",
      "iter:1300, train loss:110.42814636230469\n",
      "iter:1400, train loss:108.05924987792969\n",
      "iter:1500, train loss:85.28253936767578\n",
      "iter:1600, train loss:85.07755279541016\n",
      "iter:1700, train loss:102.66880798339844\n",
      "iter:1800, train loss:107.91878509521484\n",
      "iter:1900, train loss:133.2424774169922\n",
      "iter:2000, train loss:76.89985656738281\n",
      "iter:2100, train loss:78.38485717773438\n",
      "iter:2200, train loss:100.64167785644531\n",
      "iter:2300, train loss:80.2358627319336\n",
      "iter:2400, train loss:100.91756439208984\n",
      "iter:2500, train loss:84.763916015625\n",
      "iter:2600, train loss:97.47721862792969\n",
      "iter:2700, train loss:100.1057357788086\n",
      "iter:2800, train loss:75.78109741210938\n",
      "iter:2900, train loss:126.89140319824219\n",
      "iter:3000, train loss:78.62091827392578\n",
      "iter:3100, train loss:117.62505340576172\n",
      "iter:3200, train loss:81.96894073486328\n",
      "iter:3300, train loss:62.908512115478516\n",
      "iter:3400, train loss:103.97318267822266\n",
      "iter:3500, train loss:88.53169250488281\n",
      "iter:3600, train loss:106.28289031982422\n",
      "iter:3700, train loss:98.22766876220703\n",
      "iter:3800, train loss:92.09227752685547\n",
      "iter:3900, train loss:66.5648422241211\n",
      "iter:4000, train loss:117.70548248291016\n",
      "iter:4100, train loss:72.63294219970703\n",
      "iter:4200, train loss:83.83181762695312\n",
      "iter:4300, train loss:123.20301818847656\n",
      "iter:4400, train loss:87.93415832519531\n",
      "iter:4500, train loss:86.31226348876953\n",
      "iter:4600, train loss:89.27107238769531\n",
      "iter:4700, train loss:91.6655502319336\n",
      "iter:4800, train loss:78.2056884765625\n",
      "iter:4900, train loss:79.14979553222656\n",
      "iter:5000, train loss:107.3552474975586\n",
      "iter:5100, train loss:73.50019073486328\n",
      "iter:5200, train loss:81.85601043701172\n",
      "iter:5300, train loss:107.62200164794922\n",
      "iter:5400, train loss:85.83454132080078\n",
      "iter:5500, train loss:105.77708435058594\n",
      "iter:5600, train loss:79.84431457519531\n",
      "iter:5700, train loss:59.343868255615234\n",
      "iter:5800, train loss:116.59248352050781\n",
      "iter:5900, train loss:111.32343292236328\n",
      "iter:6000, train loss:111.93714904785156\n",
      "iter:6100, train loss:73.24049377441406\n",
      "iter:6200, train loss:110.53836822509766\n",
      "iter:6300, train loss:120.64990997314453\n",
      "iter:6400, train loss:72.82582092285156\n",
      "iter:6500, train loss:104.99886322021484\n",
      "iter:6600, train loss:113.47784423828125\n",
      "iter:6700, train loss:99.01605987548828\n",
      "iter:6800, train loss:99.14370727539062\n",
      "iter:6900, train loss:66.20065307617188\n",
      "bleu score:0.1312104468936946, loss:112.25119018554688\n",
      "max_bleu =  0.1312104468936946\n",
      "\n",
      "epoch  3 \n",
      "\n",
      "iter:0, train loss:106.44338989257812\n",
      "iter:100, train loss:81.86579132080078\n",
      "iter:200, train loss:66.21493530273438\n",
      "iter:300, train loss:103.04615783691406\n",
      "iter:400, train loss:83.87250518798828\n",
      "iter:500, train loss:127.9620132446289\n",
      "iter:600, train loss:71.7586669921875\n",
      "iter:700, train loss:98.77357482910156\n",
      "iter:800, train loss:95.55350494384766\n",
      "iter:900, train loss:90.78174591064453\n",
      "iter:1000, train loss:92.83265686035156\n",
      "iter:1100, train loss:105.03528594970703\n",
      "iter:1200, train loss:74.65544128417969\n",
      "iter:1300, train loss:82.34442138671875\n",
      "iter:1400, train loss:75.07787322998047\n",
      "iter:1500, train loss:94.98495483398438\n",
      "iter:1600, train loss:94.35763549804688\n",
      "iter:1700, train loss:83.15538024902344\n",
      "iter:1800, train loss:92.41322326660156\n",
      "iter:1900, train loss:118.91981506347656\n",
      "iter:2000, train loss:111.4879150390625\n",
      "iter:2100, train loss:103.37030029296875\n",
      "iter:2200, train loss:75.36286163330078\n",
      "iter:2300, train loss:105.94015502929688\n",
      "iter:2400, train loss:88.49051666259766\n",
      "iter:2500, train loss:113.91027069091797\n",
      "iter:2600, train loss:72.29251098632812\n",
      "iter:2700, train loss:106.2735595703125\n",
      "iter:2800, train loss:75.99127197265625\n",
      "iter:2900, train loss:65.27605438232422\n",
      "iter:3000, train loss:79.50837707519531\n",
      "iter:3100, train loss:97.3212661743164\n",
      "iter:3200, train loss:102.38086700439453\n",
      "iter:3300, train loss:69.94867706298828\n",
      "iter:3400, train loss:93.43109130859375\n",
      "iter:3500, train loss:92.5735092163086\n",
      "iter:3600, train loss:78.86068725585938\n",
      "iter:3700, train loss:57.074031829833984\n",
      "iter:3800, train loss:70.51752471923828\n",
      "iter:3900, train loss:85.34065246582031\n",
      "iter:4000, train loss:78.45470428466797\n",
      "iter:4100, train loss:110.26345825195312\n",
      "iter:4200, train loss:69.71429443359375\n",
      "iter:4300, train loss:95.13203430175781\n",
      "iter:4400, train loss:100.578369140625\n",
      "iter:4500, train loss:101.10186767578125\n",
      "iter:4600, train loss:63.263328552246094\n",
      "iter:4700, train loss:66.94612121582031\n",
      "iter:4800, train loss:78.84725952148438\n",
      "iter:4900, train loss:68.7818832397461\n",
      "iter:5000, train loss:109.03619384765625\n",
      "iter:5100, train loss:61.825496673583984\n",
      "iter:5200, train loss:82.39373779296875\n",
      "iter:5300, train loss:90.85880279541016\n",
      "iter:5400, train loss:78.41909790039062\n",
      "iter:5500, train loss:92.44625854492188\n",
      "iter:5600, train loss:103.54542541503906\n",
      "iter:5700, train loss:90.10884094238281\n",
      "iter:5800, train loss:76.53422546386719\n",
      "iter:5900, train loss:78.82719421386719\n",
      "iter:6000, train loss:109.82987976074219\n",
      "iter:6100, train loss:94.80206298828125\n",
      "iter:6200, train loss:75.99325561523438\n",
      "iter:6300, train loss:106.92185974121094\n",
      "iter:6400, train loss:68.23806762695312\n",
      "iter:6500, train loss:125.38455200195312\n",
      "iter:6600, train loss:104.43236541748047\n",
      "iter:6700, train loss:69.3666763305664\n",
      "iter:6800, train loss:104.16814422607422\n",
      "iter:6900, train loss:100.40324401855469\n",
      "bleu score:0.13701525209228377, loss:115.001953125\n",
      "max_bleu =  0.13701525209228377\n",
      "\n",
      "epoch  4 \n",
      "\n",
      "iter:0, train loss:106.51077270507812\n",
      "iter:100, train loss:69.00658416748047\n",
      "iter:200, train loss:79.5263442993164\n",
      "iter:300, train loss:79.98168182373047\n",
      "iter:400, train loss:84.79728698730469\n",
      "iter:500, train loss:101.46288299560547\n",
      "iter:600, train loss:58.99936294555664\n",
      "iter:700, train loss:96.14128112792969\n",
      "iter:800, train loss:99.7961196899414\n",
      "iter:900, train loss:96.48072052001953\n",
      "iter:1000, train loss:80.75763702392578\n",
      "iter:1100, train loss:91.16435241699219\n",
      "iter:1200, train loss:86.47683715820312\n",
      "iter:1300, train loss:102.74954223632812\n",
      "iter:1400, train loss:74.354736328125\n",
      "iter:1500, train loss:96.76404571533203\n",
      "iter:1600, train loss:65.54810333251953\n",
      "iter:1700, train loss:79.3987808227539\n",
      "iter:1800, train loss:102.74772644042969\n",
      "iter:1900, train loss:75.07819366455078\n",
      "iter:2000, train loss:69.15257263183594\n",
      "iter:2100, train loss:96.3951187133789\n",
      "iter:2200, train loss:107.63528442382812\n",
      "iter:2300, train loss:99.00883483886719\n",
      "iter:2400, train loss:78.35758972167969\n",
      "iter:2500, train loss:97.33204650878906\n",
      "iter:2600, train loss:98.58475494384766\n",
      "iter:2700, train loss:117.93275451660156\n",
      "iter:2800, train loss:95.16282653808594\n",
      "iter:2900, train loss:91.25299072265625\n",
      "iter:3000, train loss:104.03671264648438\n",
      "iter:3100, train loss:112.56925964355469\n",
      "iter:3200, train loss:76.80791473388672\n",
      "iter:3300, train loss:98.39127349853516\n",
      "iter:3400, train loss:68.75094604492188\n",
      "iter:3500, train loss:106.95840454101562\n",
      "iter:3600, train loss:104.08561706542969\n",
      "iter:3700, train loss:104.97069549560547\n",
      "iter:3800, train loss:94.93270874023438\n",
      "iter:3900, train loss:104.1340560913086\n",
      "iter:4000, train loss:109.93583679199219\n",
      "iter:4100, train loss:82.16253662109375\n",
      "iter:4200, train loss:85.19083404541016\n",
      "iter:4300, train loss:74.87273406982422\n",
      "iter:4400, train loss:101.27983856201172\n",
      "iter:4500, train loss:85.56797790527344\n",
      "iter:4600, train loss:71.66195678710938\n",
      "iter:4700, train loss:119.11453247070312\n",
      "iter:4800, train loss:96.95732116699219\n",
      "iter:4900, train loss:97.18109130859375\n",
      "iter:5000, train loss:134.12644958496094\n",
      "iter:5100, train loss:66.46170806884766\n",
      "iter:5200, train loss:73.19448852539062\n",
      "iter:5300, train loss:110.00643157958984\n",
      "iter:5400, train loss:82.24214172363281\n",
      "iter:5500, train loss:101.61400604248047\n",
      "iter:5600, train loss:72.29669952392578\n",
      "iter:5700, train loss:92.52658081054688\n",
      "iter:5800, train loss:70.94734191894531\n",
      "iter:5900, train loss:103.27234649658203\n",
      "iter:6000, train loss:63.70216751098633\n",
      "iter:6100, train loss:98.85746765136719\n",
      "iter:6200, train loss:79.0439224243164\n",
      "iter:6300, train loss:113.07383728027344\n",
      "iter:6400, train loss:92.26518249511719\n",
      "iter:6500, train loss:111.44637298583984\n",
      "iter:6600, train loss:67.32103729248047\n",
      "iter:6700, train loss:111.6327133178711\n",
      "iter:6800, train loss:83.39674377441406\n",
      "iter:6900, train loss:76.97376251220703\n",
      "bleu score:0.14417780302694275, loss:110.52947235107422\n",
      "max_bleu =  0.14417780302694275\n",
      "\n",
      "epoch  5 \n",
      "\n",
      "iter:0, train loss:72.94895935058594\n",
      "iter:100, train loss:81.24129486083984\n",
      "iter:200, train loss:101.247802734375\n",
      "iter:300, train loss:108.20460510253906\n",
      "iter:400, train loss:112.09854125976562\n",
      "iter:500, train loss:99.43709564208984\n",
      "iter:600, train loss:71.26297760009766\n",
      "iter:700, train loss:104.45677185058594\n",
      "iter:800, train loss:61.65917205810547\n",
      "iter:900, train loss:101.7249755859375\n",
      "iter:1000, train loss:111.35296630859375\n",
      "iter:1100, train loss:65.3116226196289\n",
      "iter:1200, train loss:70.2002182006836\n",
      "iter:1300, train loss:62.92170715332031\n",
      "iter:1400, train loss:120.48811340332031\n",
      "iter:1500, train loss:77.2765884399414\n",
      "iter:1600, train loss:95.10325622558594\n",
      "iter:1700, train loss:64.6895751953125\n",
      "iter:1800, train loss:66.56249237060547\n",
      "iter:1900, train loss:103.75660705566406\n",
      "iter:2000, train loss:98.90408325195312\n",
      "iter:2100, train loss:63.27180480957031\n",
      "iter:2200, train loss:65.87161254882812\n",
      "iter:2300, train loss:105.67215728759766\n",
      "iter:2400, train loss:103.27136993408203\n",
      "iter:2500, train loss:81.9150619506836\n",
      "iter:2600, train loss:100.37846374511719\n",
      "iter:2700, train loss:99.71975708007812\n",
      "iter:2800, train loss:98.27432250976562\n",
      "iter:2900, train loss:75.20105743408203\n",
      "iter:3000, train loss:92.9892578125\n",
      "iter:3100, train loss:38.92687225341797\n",
      "iter:3200, train loss:101.63621520996094\n",
      "iter:3300, train loss:71.03874206542969\n",
      "iter:3400, train loss:81.81632995605469\n",
      "iter:3500, train loss:76.897705078125\n",
      "iter:3600, train loss:76.37161254882812\n",
      "iter:3700, train loss:90.40702056884766\n",
      "iter:3800, train loss:73.5403060913086\n",
      "iter:3900, train loss:85.90177154541016\n",
      "iter:4000, train loss:81.45040893554688\n",
      "iter:4100, train loss:104.91592407226562\n",
      "iter:4200, train loss:71.54811096191406\n",
      "iter:4300, train loss:72.43211364746094\n",
      "iter:4400, train loss:71.48785400390625\n",
      "iter:4500, train loss:78.63937377929688\n",
      "iter:4600, train loss:65.00218200683594\n",
      "iter:4700, train loss:81.84979248046875\n",
      "iter:4800, train loss:66.35736846923828\n",
      "iter:4900, train loss:73.64574432373047\n",
      "iter:5000, train loss:88.06273651123047\n",
      "iter:5100, train loss:93.2811279296875\n",
      "iter:5200, train loss:67.66848754882812\n",
      "iter:5300, train loss:99.14642333984375\n",
      "iter:5400, train loss:82.5123291015625\n",
      "iter:5500, train loss:98.52640533447266\n",
      "iter:5600, train loss:69.14539337158203\n",
      "iter:5700, train loss:58.51273727416992\n",
      "iter:5800, train loss:74.5230712890625\n",
      "iter:5900, train loss:68.18900299072266\n",
      "iter:6000, train loss:69.41964721679688\n",
      "iter:6100, train loss:95.70374298095703\n",
      "iter:6200, train loss:84.92918395996094\n",
      "iter:6300, train loss:87.70402526855469\n",
      "iter:6400, train loss:58.97388458251953\n",
      "iter:6500, train loss:94.20366668701172\n",
      "iter:6600, train loss:84.10401153564453\n",
      "iter:6700, train loss:100.18486022949219\n",
      "iter:6800, train loss:71.81562805175781\n",
      "iter:6900, train loss:88.07148742675781\n",
      "bleu score:0.14641042244689606, loss:109.51386260986328\n",
      "WARNING:tensorflow:From C:\\Users\\19843\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "max_bleu =  0.14641042244689606\n",
      "\n",
      "epoch  6 \n",
      "\n",
      "iter:0, train loss:84.87060546875\n",
      "iter:100, train loss:73.47882843017578\n",
      "iter:200, train loss:81.57379150390625\n",
      "iter:300, train loss:84.23737335205078\n",
      "iter:400, train loss:64.99723815917969\n",
      "iter:500, train loss:99.52638244628906\n",
      "iter:600, train loss:86.76177978515625\n",
      "iter:700, train loss:70.90501403808594\n",
      "iter:800, train loss:102.96574401855469\n",
      "iter:900, train loss:87.93740844726562\n",
      "iter:1000, train loss:111.07563781738281\n",
      "iter:1100, train loss:63.11491394042969\n",
      "iter:1200, train loss:89.35469818115234\n",
      "iter:1300, train loss:75.25132751464844\n",
      "iter:1400, train loss:88.93267059326172\n",
      "iter:1500, train loss:100.53734588623047\n",
      "iter:1600, train loss:91.96060180664062\n",
      "iter:1700, train loss:111.3906021118164\n",
      "iter:1800, train loss:99.76483154296875\n",
      "iter:1900, train loss:110.78042602539062\n",
      "iter:2000, train loss:74.2941665649414\n",
      "iter:2100, train loss:90.16243743896484\n",
      "iter:2200, train loss:94.35929107666016\n",
      "iter:2300, train loss:101.04515075683594\n",
      "iter:2400, train loss:91.9070816040039\n",
      "iter:2500, train loss:64.19943237304688\n",
      "iter:2600, train loss:111.16470336914062\n",
      "iter:2700, train loss:90.8334732055664\n",
      "iter:2800, train loss:92.4160385131836\n",
      "iter:2900, train loss:104.2037353515625\n",
      "iter:3000, train loss:90.18292236328125\n",
      "iter:3100, train loss:68.10612487792969\n",
      "iter:3200, train loss:97.79777526855469\n",
      "iter:3300, train loss:76.56510925292969\n",
      "iter:3400, train loss:74.86127471923828\n",
      "iter:3500, train loss:72.46354675292969\n",
      "iter:3600, train loss:91.20449829101562\n",
      "iter:3700, train loss:100.04716491699219\n",
      "iter:3800, train loss:93.41177368164062\n",
      "iter:3900, train loss:101.34832763671875\n",
      "iter:4000, train loss:80.91305541992188\n",
      "iter:4100, train loss:86.43021392822266\n",
      "iter:4200, train loss:102.7076187133789\n",
      "iter:4300, train loss:66.09967803955078\n",
      "iter:4400, train loss:96.71673583984375\n",
      "iter:4500, train loss:91.96172332763672\n",
      "iter:4600, train loss:85.05534362792969\n",
      "iter:4700, train loss:73.36080932617188\n",
      "iter:4800, train loss:92.23028564453125\n",
      "iter:4900, train loss:89.69087219238281\n",
      "iter:5000, train loss:70.0021743774414\n",
      "iter:5100, train loss:102.40898132324219\n",
      "iter:5200, train loss:68.93648529052734\n",
      "iter:5300, train loss:98.71273803710938\n",
      "iter:5400, train loss:105.4544906616211\n",
      "iter:5500, train loss:92.42605590820312\n",
      "iter:5600, train loss:89.1619644165039\n",
      "iter:5700, train loss:111.99173736572266\n",
      "iter:5800, train loss:73.80111694335938\n",
      "iter:5900, train loss:96.73490905761719\n",
      "iter:6000, train loss:90.69212341308594\n",
      "iter:6100, train loss:97.77936553955078\n",
      "iter:6200, train loss:73.504638671875\n",
      "iter:6300, train loss:105.50238037109375\n",
      "iter:6400, train loss:95.56588745117188\n",
      "iter:6500, train loss:103.7949447631836\n",
      "iter:6600, train loss:97.33749389648438\n",
      "iter:6700, train loss:61.67853546142578\n",
      "iter:6800, train loss:89.86494445800781\n",
      "iter:6900, train loss:92.0793685913086\n",
      "bleu score:0.1479018406623011, loss:107.08229064941406\n",
      "max_bleu =  0.1479018406623011\n",
      "\n",
      "epoch  7 \n",
      "\n",
      "iter:0, train loss:95.61479949951172\n",
      "iter:100, train loss:81.61347961425781\n",
      "iter:200, train loss:81.51115417480469\n",
      "iter:300, train loss:110.30665588378906\n",
      "iter:400, train loss:80.47860717773438\n",
      "iter:500, train loss:57.19956970214844\n",
      "iter:600, train loss:87.83243560791016\n",
      "iter:700, train loss:90.72386932373047\n",
      "iter:800, train loss:96.8277816772461\n",
      "iter:900, train loss:76.04491424560547\n",
      "iter:1000, train loss:67.06767272949219\n",
      "iter:1100, train loss:88.4098892211914\n",
      "iter:1200, train loss:88.96705627441406\n",
      "iter:1300, train loss:68.70187377929688\n",
      "iter:1400, train loss:72.3699722290039\n",
      "iter:1500, train loss:98.68508911132812\n",
      "iter:1600, train loss:88.9940414428711\n",
      "iter:1700, train loss:108.86141967773438\n",
      "iter:1800, train loss:59.285057067871094\n",
      "iter:1900, train loss:106.09324645996094\n",
      "iter:2000, train loss:102.04536437988281\n",
      "iter:2100, train loss:75.65512084960938\n",
      "iter:2200, train loss:94.61246490478516\n",
      "iter:2300, train loss:90.75932312011719\n",
      "iter:2400, train loss:97.17576599121094\n",
      "iter:2500, train loss:82.50506591796875\n",
      "iter:2600, train loss:104.49644470214844\n",
      "iter:2700, train loss:85.41146087646484\n",
      "iter:2800, train loss:93.3988037109375\n",
      "iter:2900, train loss:97.8585205078125\n",
      "iter:3000, train loss:108.90479278564453\n",
      "iter:3100, train loss:103.99015045166016\n",
      "iter:3200, train loss:70.80813598632812\n",
      "iter:3300, train loss:113.74073791503906\n",
      "iter:3400, train loss:68.51380157470703\n",
      "iter:3500, train loss:92.60492706298828\n",
      "iter:3600, train loss:82.61796569824219\n",
      "iter:3700, train loss:73.87197875976562\n",
      "iter:3800, train loss:72.59361267089844\n",
      "iter:3900, train loss:124.00619506835938\n",
      "iter:4000, train loss:102.43404388427734\n",
      "iter:4100, train loss:70.8996353149414\n",
      "iter:4200, train loss:65.279541015625\n",
      "iter:4300, train loss:90.41144561767578\n",
      "iter:4400, train loss:66.1383285522461\n",
      "iter:4500, train loss:69.45263671875\n",
      "iter:4600, train loss:94.82042694091797\n",
      "iter:4700, train loss:82.99822998046875\n",
      "iter:4800, train loss:72.79641723632812\n",
      "iter:4900, train loss:90.49151611328125\n",
      "iter:5000, train loss:87.57394409179688\n",
      "iter:5100, train loss:58.55846405029297\n",
      "iter:5200, train loss:74.93743133544922\n",
      "iter:5300, train loss:102.430419921875\n",
      "iter:5400, train loss:74.8947525024414\n",
      "iter:5500, train loss:91.71228790283203\n",
      "iter:5600, train loss:69.1725845336914\n",
      "iter:5700, train loss:72.32811737060547\n",
      "iter:5800, train loss:73.89291381835938\n",
      "iter:5900, train loss:107.17169952392578\n",
      "iter:6000, train loss:67.67306518554688\n",
      "iter:6100, train loss:87.39144897460938\n",
      "iter:6200, train loss:66.69095611572266\n",
      "iter:6300, train loss:95.13011932373047\n",
      "iter:6400, train loss:71.23081970214844\n",
      "iter:6500, train loss:64.37435150146484\n",
      "iter:6600, train loss:98.94181823730469\n",
      "iter:6700, train loss:115.96990966796875\n",
      "iter:6800, train loss:87.90165710449219\n",
      "iter:6900, train loss:73.07073974609375\n",
      "bleu score:0.15216827004657163, loss:103.49699401855469\n",
      "max_bleu =  0.15216827004657163\n",
      "\n",
      "epoch  8 \n",
      "\n",
      "iter:0, train loss:71.34859466552734\n",
      "iter:100, train loss:67.08877563476562\n",
      "iter:200, train loss:53.4111328125\n",
      "iter:300, train loss:74.49052429199219\n",
      "iter:400, train loss:59.056251525878906\n",
      "iter:500, train loss:112.18610382080078\n",
      "iter:600, train loss:71.6621322631836\n",
      "iter:700, train loss:99.94267272949219\n",
      "iter:800, train loss:97.00374603271484\n",
      "iter:900, train loss:96.35354614257812\n",
      "iter:1000, train loss:86.14532470703125\n",
      "iter:1100, train loss:53.62055969238281\n",
      "iter:1200, train loss:75.7315444946289\n",
      "iter:1300, train loss:77.43574523925781\n",
      "iter:1400, train loss:78.7842788696289\n",
      "iter:1500, train loss:64.84761047363281\n",
      "iter:1600, train loss:69.31431579589844\n",
      "iter:1700, train loss:58.01874923706055\n",
      "iter:1800, train loss:79.87366485595703\n",
      "iter:1900, train loss:94.98786926269531\n",
      "iter:2000, train loss:103.04568481445312\n",
      "iter:2100, train loss:61.85602569580078\n",
      "iter:2200, train loss:74.33545684814453\n",
      "iter:2300, train loss:75.6594467163086\n",
      "iter:2400, train loss:86.83777618408203\n",
      "iter:2500, train loss:83.9804916381836\n",
      "iter:2600, train loss:69.51927947998047\n",
      "iter:2700, train loss:71.74376678466797\n",
      "iter:2800, train loss:80.0472412109375\n",
      "iter:2900, train loss:65.36164093017578\n",
      "iter:3000, train loss:65.96115112304688\n",
      "iter:3100, train loss:106.79611206054688\n",
      "iter:3200, train loss:65.68592834472656\n",
      "iter:3300, train loss:64.33087158203125\n",
      "iter:3400, train loss:95.93965148925781\n",
      "iter:3500, train loss:90.31980895996094\n",
      "iter:3600, train loss:91.11884307861328\n",
      "iter:3700, train loss:89.57646942138672\n",
      "iter:3800, train loss:61.27033996582031\n",
      "iter:3900, train loss:109.06999206542969\n",
      "iter:4000, train loss:58.39389419555664\n",
      "iter:4100, train loss:96.82621765136719\n",
      "iter:4200, train loss:97.3647232055664\n",
      "iter:4300, train loss:98.16474914550781\n",
      "iter:4400, train loss:102.92620849609375\n",
      "iter:4500, train loss:76.4295883178711\n",
      "iter:4600, train loss:64.3622055053711\n",
      "iter:4700, train loss:103.32748413085938\n",
      "iter:4800, train loss:103.57658386230469\n",
      "iter:4900, train loss:98.55863952636719\n",
      "iter:5000, train loss:71.70499420166016\n",
      "iter:5100, train loss:68.18212890625\n",
      "iter:5200, train loss:61.712745666503906\n",
      "iter:5300, train loss:104.43730926513672\n",
      "iter:5400, train loss:67.26318359375\n",
      "iter:5500, train loss:91.28334045410156\n",
      "iter:5600, train loss:90.955322265625\n",
      "iter:5700, train loss:60.92361831665039\n",
      "iter:5800, train loss:61.57487869262695\n",
      "iter:5900, train loss:76.0916748046875\n",
      "iter:6000, train loss:62.38289260864258\n",
      "iter:6100, train loss:82.51639556884766\n",
      "iter:6200, train loss:64.75764465332031\n",
      "iter:6300, train loss:101.90682983398438\n",
      "iter:6400, train loss:100.89458465576172\n",
      "iter:6500, train loss:95.89299011230469\n",
      "iter:6600, train loss:61.1112060546875\n",
      "iter:6700, train loss:73.98139953613281\n",
      "iter:6800, train loss:98.25292205810547\n",
      "iter:6900, train loss:67.7146224975586\n",
      "bleu score:0.15302200602418994, loss:102.58560180664062\n",
      "max_bleu =  0.15302200602418994\n",
      "\n",
      "epoch  9 \n",
      "\n",
      "iter:0, train loss:74.11786651611328\n",
      "iter:100, train loss:98.26625061035156\n",
      "iter:200, train loss:96.00398254394531\n",
      "iter:300, train loss:95.3885498046875\n",
      "iter:400, train loss:68.58932495117188\n",
      "iter:500, train loss:67.33432006835938\n",
      "iter:600, train loss:60.89958190917969\n",
      "iter:700, train loss:75.22540283203125\n",
      "iter:800, train loss:77.8071060180664\n",
      "iter:900, train loss:92.58082580566406\n",
      "iter:1000, train loss:93.75042724609375\n",
      "iter:1100, train loss:90.1648178100586\n",
      "iter:1200, train loss:91.52223205566406\n",
      "iter:1300, train loss:101.02610778808594\n",
      "iter:1400, train loss:74.66844177246094\n",
      "iter:1500, train loss:106.77399444580078\n",
      "iter:1600, train loss:62.62709045410156\n",
      "iter:1700, train loss:85.01355743408203\n",
      "iter:1800, train loss:56.39426803588867\n",
      "iter:1900, train loss:104.65010070800781\n",
      "iter:2000, train loss:72.54236602783203\n",
      "iter:2100, train loss:98.40811157226562\n",
      "iter:2200, train loss:65.9012680053711\n",
      "iter:2300, train loss:79.54700469970703\n",
      "iter:2400, train loss:84.36314392089844\n",
      "iter:2500, train loss:99.20226287841797\n",
      "iter:2600, train loss:56.7270622253418\n",
      "iter:2700, train loss:64.08921813964844\n",
      "iter:2800, train loss:87.16842651367188\n",
      "iter:2900, train loss:77.9598388671875\n",
      "iter:3000, train loss:71.85936737060547\n",
      "iter:3100, train loss:96.28716278076172\n",
      "iter:3200, train loss:85.8563232421875\n",
      "iter:3300, train loss:76.55977630615234\n",
      "iter:3400, train loss:94.07173156738281\n",
      "iter:3500, train loss:94.43561553955078\n",
      "iter:3600, train loss:96.36195373535156\n",
      "iter:3700, train loss:92.66223907470703\n",
      "iter:3800, train loss:69.02369689941406\n",
      "iter:3900, train loss:80.3084716796875\n",
      "iter:4000, train loss:109.33039855957031\n",
      "iter:4100, train loss:74.0153579711914\n",
      "iter:4200, train loss:72.51234436035156\n",
      "iter:4300, train loss:76.3321304321289\n",
      "iter:4400, train loss:65.08473205566406\n",
      "iter:4500, train loss:71.12578582763672\n",
      "iter:4600, train loss:87.95636749267578\n",
      "iter:4700, train loss:104.6987075805664\n",
      "iter:4800, train loss:83.91085815429688\n",
      "iter:4900, train loss:77.76904296875\n",
      "iter:5000, train loss:58.832923889160156\n",
      "iter:5100, train loss:90.71306610107422\n",
      "iter:5200, train loss:91.3836898803711\n",
      "iter:5300, train loss:86.42181396484375\n",
      "iter:5400, train loss:71.29124450683594\n",
      "iter:5500, train loss:89.67189025878906\n",
      "iter:5600, train loss:85.68818664550781\n",
      "iter:5700, train loss:92.86943054199219\n",
      "iter:5800, train loss:83.23452758789062\n",
      "iter:5900, train loss:89.45425415039062\n",
      "iter:6000, train loss:100.21219635009766\n",
      "iter:6100, train loss:69.92914581298828\n",
      "iter:6200, train loss:102.98829650878906\n",
      "iter:6300, train loss:62.559974670410156\n",
      "iter:6400, train loss:36.63867950439453\n",
      "iter:6500, train loss:101.69651794433594\n",
      "iter:6600, train loss:70.75523376464844\n",
      "iter:6700, train loss:67.49630737304688\n",
      "iter:6800, train loss:111.62261962890625\n",
      "iter:6900, train loss:81.63136291503906\n",
      "bleu score:0.14933379661188345, loss:104.3611831665039\n",
      "max_bleu =  0.15302200602418994\n",
      "\n",
      "epoch  10 \n",
      "\n",
      "iter:0, train loss:75.30767822265625\n",
      "iter:100, train loss:74.09123229980469\n",
      "iter:200, train loss:84.42434692382812\n",
      "iter:300, train loss:81.73344421386719\n",
      "iter:400, train loss:60.33122634887695\n",
      "iter:500, train loss:69.14341735839844\n",
      "iter:600, train loss:93.31236267089844\n",
      "iter:700, train loss:70.5155029296875\n",
      "iter:800, train loss:63.742008209228516\n",
      "iter:900, train loss:74.998779296875\n",
      "iter:1000, train loss:63.594791412353516\n",
      "iter:1100, train loss:64.31884765625\n",
      "iter:1200, train loss:95.60798645019531\n",
      "iter:1300, train loss:104.36404418945312\n",
      "iter:1400, train loss:68.20299530029297\n",
      "iter:1500, train loss:64.417236328125\n",
      "iter:1600, train loss:69.30522155761719\n",
      "iter:1700, train loss:70.38394165039062\n",
      "iter:1800, train loss:96.97935485839844\n",
      "iter:1900, train loss:93.00391387939453\n",
      "iter:2000, train loss:89.16877746582031\n",
      "iter:2100, train loss:77.29957580566406\n",
      "iter:2200, train loss:55.68378448486328\n",
      "iter:2300, train loss:69.23579406738281\n",
      "iter:2400, train loss:72.27815246582031\n",
      "iter:2500, train loss:99.9817886352539\n",
      "iter:2600, train loss:89.51947021484375\n",
      "iter:2700, train loss:69.44197082519531\n",
      "iter:2800, train loss:88.6952133178711\n",
      "iter:2900, train loss:54.611534118652344\n",
      "iter:3000, train loss:92.05110168457031\n",
      "iter:3100, train loss:89.31695556640625\n",
      "iter:3200, train loss:93.53652954101562\n",
      "iter:3300, train loss:73.88256072998047\n",
      "iter:3400, train loss:104.2828369140625\n",
      "iter:3500, train loss:92.94342041015625\n",
      "iter:3600, train loss:86.92913818359375\n",
      "iter:3700, train loss:72.77579498291016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f23c21c7cab3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-e953f40fb2e6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    287\u001b[0m                                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY_out_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mY_out_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnow_lr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m                                 model.dropout:dropout})\n\u001b[0m\u001b[0;32m    290\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0miter_n\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iter:{}, train loss:{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNQfg25GX564"
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    with open('data/preprocess/vocab_dict.pkl', 'rb') as fr:\n",
    "        en_word2id, en_id2word, ch_word2id, ch_id2word = pkl.load(fr)\n",
    "    if type(X) == str:\n",
    "        X = X\n",
    "    elif type(X) == list or type(X) == tuple:\n",
    "        X = '\\n'.join(X)\n",
    "    else:\n",
    "        raise ValueError('You must ensure the `X` be string or list!')\n",
    "    X = segment(X, jieba.cut)\n",
    "    X = transform(X, en_word2id)\n",
    "    X = padding(X, src_max_seq_len)\n",
    "    X_len = np.sum((X > 0), axis=1)\n",
    "    # X -> (src_max_seq_len, ) or (batch, sec_max_seq_len, )\n",
    "    with tf.Session(config=cf) as sess:\n",
    "        model = NMTModel(src_max_vocab_size=src_max_vocab_size, \n",
    "                             tgt_max_vocab_size=tgt_max_vocab_size, \n",
    "                             embedding_size=embedding_size,\n",
    "                             hidden_size=hidden_size,\n",
    "                             src_max_seq_len=src_max_seq_len,\n",
    "                             tgt_max_seq_len=tgt_max_seq_len,\n",
    "                             tgt_start_id=tgt_start_id,\n",
    "                             tgt_end_id=tgt_end_id,\n",
    "                             max_gradient_norm=max_gradient_norm,\n",
    "                             maximum_iterations=maximum_iterations,\n",
    "                             optimizer=optimizer)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('model/'))\n",
    "        translations = sess.run(model.translations, \n",
    "                            feed_dict={ model.X:X,\n",
    "                                        model.Y_out:[[]],\n",
    "                                        model.Y_in:[[]], \n",
    "                                        model.X_len:X_len,\n",
    "                                        model.Y_in_len:[],\n",
    "                                        model.Y_out_len:[],\n",
    "                                        model.lr:lr,\n",
    "                                        model.dropout:0.})\n",
    "        translations = transform2word(translations, ch_id2word)\n",
    "    return translations\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NMT.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
