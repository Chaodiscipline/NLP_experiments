{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import jieba\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理，中英文分开pickle保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57325\n",
      "4882\n"
     ]
    }
   ],
   "source": [
    "def data_preprocess(path):\n",
    "    corpus_en = []\n",
    "    corpus_ch = []   \n",
    "    with open(path, 'r',encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f): # i从0开始\n",
    "            line = line.strip()\n",
    "#             .decode('utf8')\n",
    "            if i%2 ==0:\n",
    "                corpus_en.append(line)\n",
    "            else:\n",
    "                corpus_ch.append(line)\n",
    "                \n",
    "    corpus_en = '\\n'.join(corpus_en)\n",
    "    corpus_ch = '\\n'.join(corpus_ch)\n",
    "    return corpus_en, corpus_ch\n",
    "\n",
    "train_en, train_ch = data_preprocess(r\"data/train.txt\")\n",
    "valid_en, valid_ch = data_preprocess(r\"data/valid.txt\")\n",
    "\n",
    "def segment(corpus, tokenizer, savepath=None):\n",
    "    tokenized_corpus = []\n",
    "    tokenized_corpus = ' '.join([_ for _ in tokenizer(corpus) if _.strip(' ')])\n",
    "    tokenized_corpus = tokenized_corpus.split(' \\n ')\n",
    "    if savepath:\n",
    "        with open(savepath,'wb+') as fw:\n",
    "            pkl.dump(tokenized_corpus, fw)\n",
    "    return tokenized_corpus\n",
    "\n",
    "train_en = segment(train_en, jieba.cut, 'data/preprocess/train_en_segment.pkl')\n",
    "train_ch = segment(train_ch, lambda k: iter(k.strip()), 'data/preprocess/train_ch_segment.pkl')\n",
    "valid_en = segment(valid_en, jieba.cut, 'data/preprocess/valid_en_segment.pkl')\n",
    "valid_ch = segment(valid_ch, lambda k: iter(k.strip()), 'data/preprocess/valid_ch_segment.pkl') #暂时分字\n",
    "\n",
    "def vocab(data, topK=None):\n",
    "    word2id = Counter() # Counter是dict的一个子类\n",
    "    for sentence in data:\n",
    "        for word in sentence.split():\n",
    "            word2id[word] += 1\n",
    "    word2id = word2id.most_common() #按频率从高到低排序\n",
    "    if topK:\n",
    "        word2id = word2id[:topK]\n",
    "    word2id, _ = zip(*word2id)\n",
    "    word2id = {word : i + 4 for i, word in enumerate(word2id)}  # 将0，1，2，3的位置空出来\n",
    "    word2id['<PAD>'] = 0 # 中止id\n",
    "    word2id['<UNK>'] = 1 # 字典中没有该字/词的id表示\n",
    "    word2id['<S>'] = 2 # 起始id\n",
    "    word2id['</S>'] = 3\n",
    "    id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "    return word2id, id2word\n",
    "\n",
    "en_word2id, en_id2word = vocab(train_en+valid_en)\n",
    "ch_word2id, ch_id2word = vocab(train_ch+valid_ch)\n",
    "\n",
    "# 把数据中的字/词转成对应id\n",
    "print(len(en_word2id))\n",
    "print(len(ch_word2id))\n",
    "\n",
    "def transform(data, word2id):\n",
    "    ret_data = []\n",
    "    for sentence in data:\n",
    "        ret_data.append([word2id.get(word, 1) for word in sentence.split()]) # word2id.get(word, 1) 如果word不在字典中，则返回默认值1\n",
    "    return ret_data\n",
    "\n",
    "\n",
    "train_en_corpus = transform(train_en, en_word2id)\n",
    "train_ch_corpus = transform(train_ch, ch_word2id)\n",
    "valid_en_corpus = transform(valid_en, en_word2id)\n",
    "valid_ch_corpus = transform(valid_ch, ch_word2id)\n",
    "with open('data/preprocess/vocab_dict.pkl', 'wb+') as fw:\n",
    "    pkl.dump([en_word2id, en_id2word, ch_word2id, ch_id2word], fw)\n",
    "with open('data/preprocess/vocab_dict_and_corpus.pkl', 'wb+') as fw:\n",
    "    pkl.dump([en_word2id, en_id2word, ch_word2id, ch_id2word, train_en_corpus, train_ch_corpus, valid_en_corpus, valid_ch_corpus], fw)\n",
    "with open('data/preprocess/demo_vocab_dict_and_corpus.pkl', 'wb+') as fw:\n",
    "    pkl.dump([en_word2id, en_id2word, ch_word2id, ch_id2word, train_en_corpus[:100], train_ch_corpus[:100], valid_en_corpus[:100], valid_ch_corpus[:100]], fw)\n",
    "    \n",
    "# padding\n",
    "def padding(data, max_len):\n",
    "    # 对长度小于max的以0填充，大于的截断，\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(data, max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The function of a customer is defined by assigning an account group .', \"I argued a book ' s assigning us to read is no good .\"]\n",
      "['All that remains for me to do is to say good - bye .', 'All the commune members , young and old , went out to hervest the crops .']\n",
      "['All that remains for me to do is to say good - bye .', 'All the commune members , young and old , went out to hervest the crops .', 'The function of a customer is defined by assigning an account group .', \"I argued a book ' s assigning us to read is no good .\"]\n"
     ]
    }
   ],
   "source": [
    "print(valid_en[:2])\n",
    "print(train_en[:2])\n",
    "print(train_en[:2]+valid_en[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "155\n",
      "30\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "print(max(len(i) for i in train_en_corpus))\n",
    "print(max(len(i) for i in train_ch_corpus))\n",
    "print(max(len(i) for i in valid_en_corpus))\n",
    "print(max(len(i) for i in valid_ch_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_max_seq_len = 40\n",
    "# tgt_max_seq_len = 50\n",
    "# src_embedding_size = 200\n",
    "# tgt_embedding_size = 200\n",
    "# train_size = 0.8\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 28\n",
    "lr = 1e-3\n",
    "optimizer = 'adam'\n",
    "dropout = 0.2\n",
    "\n",
    "src_max_vocab_size = 57330\n",
    "tgt_max_vocab_size = 4887\n",
    "embedding_size = 128\n",
    "hidden_size = 256\n",
    "src_max_seq_len = 50\n",
    "tgt_max_seq_len = 80\n",
    "tgt_start_id = 2 # <S> \n",
    "tgt_end_id = 0 # <PAD>\n",
    "max_gradient_norm = 1.\n",
    "maximum_iterations = 40\n",
    "cf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "cf.gpu_options.per_process_gpu_memory_fraction = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Iterator(object):\n",
    "    \"\"\"\n",
    "    数据迭代器\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):  # train_en_corpus, train_ch_corpus\n",
    "        self.x = np.asarray(x)\n",
    "        self.y = np.asarray(y)\n",
    "        self.sample_num = self.x.shape[0]\n",
    "\n",
    "    def next_batch(self, batch_size): # 用于train\n",
    "        # produce X, Y_out, Y_in, X_len, Y_in_len, Y_out_len\n",
    "        l = np.random.randint(0, self.sample_num - batch_size + 1)\n",
    "        r = l + batch_size\n",
    "        x_part = self.x[l:r]\n",
    "        y_out_part = self.y[l:r]\n",
    "        x_len = np.sum((x_part > 0), axis=1)\n",
    "        y_in_part = np.concatenate((np.ones((batch_size, 1)) * 2, y_out_part[:,:-1]), axis=-1)\n",
    "        max_y_dim = self.y.shape[1]\n",
    "        y_out_len = np.sum((y_out_part > 0), axis=1) + 1\n",
    "        y_out_len = np.min(np.concatenate([np.ones((batch_size, 1)) * max_y_dim, y_out_len.reshape(-1, 1)], axis=-1), axis=-1)\n",
    "        y_in_len = np.sum((y_in_part > 0), axis=1) + 1\n",
    "        y_in_len = np.min(np.concatenate([np.ones((batch_size, 1)) * max_y_dim, y_in_len.reshape(-1, 1)], axis=-1), axis=-1)\n",
    "        x_len = x_len.astype(np.int32)\n",
    "        y_in_part = y_in_part.astype(np.int32)\n",
    "        y_in_len = y_in_len.astype(np.int32)\n",
    "        y_out_len = y_out_len.astype(np.int32)\n",
    "        return x_part, y_out_part, y_in_part, x_len, y_in_len, y_out_len\n",
    "\n",
    "    def next(self, batch_size): #用于evaluate\n",
    "        l = 0\n",
    "        while l < self.sample_num:\n",
    "            r = min(l + batch_size, self.sample_num)\n",
    "            batch_size = r - l\n",
    "            x_part = self.x[l:r] # 没有要第一句\n",
    "            y_out_part = self.y[l:r]\n",
    "            x_len = np.sum((x_part > 0), axis=1) # 去掉填充的0之后，每条句子的长度\n",
    "            y_in_part = np.concatenate((np.ones((batch_size, 1)) * 2, y_out_part[:,:-1]), axis=-1) # 在第一列加了一列 2\n",
    "            max_y_dim = self.y.shape[1] # 句子长度\n",
    "            \n",
    "            y_out_len = np.sum((y_out_part > 0), axis=1) + 1\n",
    "            # 每条句子返回max_y_dim 与 y_out_part句子长度的较小值\n",
    "            y_out_len = np.min(np.concatenate([np.ones((batch_size, 1)) * max_y_dim, y_out_len.reshape(-1, 1)], axis=-1), axis=-1) # reshape(-1, 1) 表示行数未知，列数为1\n",
    "            y_in_len = np.sum((y_in_part > 0), axis=1) + 1\n",
    "            y_in_len = np.min(np.concatenate([np.ones((batch_size, 1)) * max_y_dim, y_in_len.reshape(-1, 1)], axis=-1), axis=-1)\n",
    "            \n",
    "            x_len = x_len.astype(np.int32)\n",
    "            y_in_part = y_in_part.astype(np.int32)\n",
    "            y_in_len = y_in_len.astype(np.int32)\n",
    "            y_out_len = y_out_len.astype(np.int32)\n",
    "            l += batch_size\n",
    "            yield x_part, y_out_part, y_in_part, x_len, y_in_len, y_out_len\n",
    "            \n",
    "            \n",
    "            \n",
    "class NMTModel(object):\n",
    "    \"\"\"\n",
    "    带Attention的NMT模型\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 src_max_vocab_size, \n",
    "                 tgt_max_vocab_size, \n",
    "                 embedding_size,\n",
    "                 hidden_size,\n",
    "                 src_max_seq_len,\n",
    "                 tgt_max_seq_len,\n",
    "                 tgt_start_id,\n",
    "                 tgt_end_id,\n",
    "                 max_gradient_norm=5,\n",
    "                 maximum_iterations=None,\n",
    "                 optimizer='adam',\n",
    "                 ):\n",
    "        self.initializer = tf.random_uniform_initializer(\n",
    "        -0.05, 0.05)\n",
    "        self.optimizer = optimizer\n",
    "        # 源词表大小\n",
    "        self.src_max_vocab_size = src_max_vocab_size\n",
    "        # 目标词表大小\n",
    "        self.tgt_max_vocab_size = tgt_max_vocab_size\n",
    "        # 输入embedding大小（src与tgt的embedding_size可以不同）\n",
    "        self.embedding_size = embedding_size\n",
    "        # 隐层大小\n",
    "        self.hidden_size = hidden_size\n",
    "        # 源序列长度\n",
    "        self.src_max_seq_len = src_max_seq_len\n",
    "        # 目标序列长度\n",
    "        self.tgt_max_seq_len = tgt_max_seq_len\n",
    "        # 目标序列起始id（输入的初始id值）\n",
    "        self.tgt_start_id = tgt_start_id\n",
    "        # 目标的终结id（模型预测到该id后停止预测）\n",
    "        self.tgt_end_id = tgt_end_id\n",
    "        if maximum_iterations is None:\n",
    "            self.maximum_iterations = self.tgt_max_seq_len\n",
    "        else:\n",
    "            self.maximum_iterations = maximum_iterations\n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "        self.add_placeholders()\n",
    "        self.batch_size = tf.shape(self.X)[0]\n",
    "        self.add_embeddings()\n",
    "        self.encoder()\n",
    "        self.decoder()\n",
    "        self.add_loss()\n",
    "        self.add_train_op()\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        # X, Y_out, Y_in, X_len, Y_in_len, Y_out_len\n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y_out = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y_in = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_len = tf.placeholder(tf.int32, [None, ])\n",
    "        self.Y_in_len = tf.placeholder(tf.int32, [None, ])\n",
    "        self.Y_out_len = tf.placeholder(tf.int32, [None, ])\n",
    "        self.lr = tf.placeholder(tf.float32)\n",
    "        self.dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "    def add_embeddings(self):\n",
    "        with tf.variable_scope('embeddings', initializer=self.initializer):\n",
    "            # 创建变量\n",
    "            self.X_emb = tf.get_variable('X_emb', \n",
    "                shape=(self.src_max_vocab_size, self.embedding_size), \n",
    "                dtype=tf.float32)\n",
    "            self.Y_emb = tf.get_variable('Y_emb', \n",
    "                shape=(self.tgt_max_vocab_size, self.embedding_size), \n",
    "                dtype=tf.float32)\n",
    "\n",
    "            self.encoder_input = tf.nn.embedding_lookup(self.X_emb, self.X)\n",
    "            self.decoder_input = tf.nn.embedding_lookup(self.Y_emb, self.Y_in)\n",
    "\n",
    "    def encoder(self):\n",
    "        with tf.variable_scope('encoder'):\n",
    "            fw_encoder_cell = tf.contrib.rnn.GRUCell(self.hidden_size)\n",
    "            fw_encoder_cell = tf.contrib.rnn.DropoutWrapper(fw_encoder_cell, input_keep_prob=1-self.dropout)\n",
    "            bw_encoder_cell = tf.contrib.rnn.GRUCell(self.hidden_size)\n",
    "            bw_encoder_cell = tf.contrib.rnn.DropoutWrapper(bw_encoder_cell, input_keep_prob=1-self.dropout)\n",
    "\n",
    "            # 双向RNN\n",
    "            encoder_outputs, bi_last_state = tf.nn.bidirectional_dynamic_rnn(     \n",
    "                    fw_encoder_cell, bw_encoder_cell, self.encoder_input, \n",
    "                    self.X_len, dtype=tf.float32)\n",
    "            self.encoder_outputs = tf.concat(encoder_outputs, axis=-1)\n",
    "            self.encoder_last_state = bi_last_state\n",
    "\n",
    "\n",
    "    def decoder(self):\n",
    "        with tf.variable_scope('decoder'):\n",
    "            decoder_cell = tf.contrib.rnn.GRUCell(self.hidden_size)\n",
    "            decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=1-self.dropout)\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                                    self.hidden_size, self.encoder_outputs,\n",
    "                                    memory_sequence_length=self.X_len)\n",
    "            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                                    decoder_cell, attention_mechanism,\n",
    "                                    attention_layer_size=self.hidden_size)\n",
    "\n",
    "            projection_layer = layers_core.Dense(\n",
    "            self.tgt_max_vocab_size, use_bias=False)\n",
    "\n",
    "        # 训练或评估的时候，decoder的output是真实的target，input是target右移一个词\n",
    "        with tf.variable_scope('dynamic_decode'):\n",
    "            # Helper\n",
    "            helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                self.decoder_input, tf.ones((self.batch_size, ), dtype=tf.int32) * self.tgt_max_seq_len, time_major=False)\n",
    "            # Decoder\n",
    "            decoder_initial_state = decoder_cell.zero_state(self.batch_size, dtype=tf.float32).clone(\n",
    "                cell_state=self.encoder_last_state[0])\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                decoder_cell, helper, decoder_initial_state,\n",
    "                output_layer=projection_layer)\n",
    "            # Dynamic decoding\n",
    "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "            self.logits = outputs.rnn_output\n",
    "            self.pred = tf.argmax(self.logits, axis=2)\n",
    "\n",
    "        # 预测的时候，decoder的每个timestep的输入为前一个时刻的输出\n",
    "        with tf.variable_scope('dynamic_decode', reuse=True):\n",
    "            # Helper\n",
    "            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                self.Y_emb,\n",
    "                start_tokens=tf.fill([self.batch_size], self.tgt_start_id),\n",
    "                end_token=self.tgt_end_id)\n",
    "            decoder_initial_state = decoder_cell.zero_state(self.batch_size, dtype=tf.float32).clone(\n",
    "                cell_state=self.encoder_last_state[0])\n",
    "            # Decoder\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                decoder_cell, helper, decoder_initial_state,\n",
    "                output_layer=projection_layer)\n",
    "            # Dynamic decoding\n",
    "            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder, maximum_iterations=self.maximum_iterations)\n",
    "            self.translations = outputs.sample_id\n",
    "\n",
    "    def add_loss(self): # 交叉熵\n",
    "        target_weights = tf.sequence_mask(\n",
    "                         self.Y_out_len, self.tgt_max_seq_len, dtype=self.logits.dtype)\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                         labels=self.Y_out, logits=self.logits)\n",
    "        self.loss_op = (tf.reduce_sum(crossent * target_weights) / tf.to_float(self.batch_size))\n",
    "\n",
    "    def add_train_op(self):   # 优化器\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(self.loss_op, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(\n",
    "            gradients, self.max_gradient_norm)\n",
    "        # Optimization\n",
    "        if self.optimizer == 'sgd':\n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        elif self.optimizer == 'adadelta':\n",
    "            optimizer = tf.train.AdaDeltaOptimizer(self.lr)\n",
    "        else:\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(clipped_gradients, params), global_step=self.global_step)\n",
    "        \n",
    "        \n",
    "def transform2word(data, id2word):\n",
    "    \"\"\"\n",
    "    把id转成word\n",
    "    \"\"\"\n",
    "    ret_data = []\n",
    "    for sentence in data:\n",
    "        ret_data.append(''.join([id2word.get(word, '<UNK>') for word in sentence]))\n",
    "    return ret_data\n",
    "\n",
    "def bleu(refs, hyps):\n",
    "    \"\"\"\n",
    "    计算bleu-4\n",
    "    \"\"\"\n",
    "    refs = [[[_ for _ in ref if _ > 0]] for ref in refs]\n",
    "    hyps = [[_ for _ in hyp if _ > 0] for hyp in hyps]\n",
    "    return nltk.translate.bleu_score.corpus_bleu(refs, hyps)\n",
    "\n",
    "def train():\n",
    "    # load data and dictionary\n",
    "    with open('data/preprocess/vocab_dict_and_corpus.pkl', 'rb') as fr:\n",
    "        en_word2id, en_id2word, ch_word2id, ch_id2word, \\\n",
    "        train_en_corpus, train_ch_corpus, eval_en_corpus, eval_ch_corpus = pkl.load(fr)\n",
    "\n",
    "    train_en_corpus = padding(train_en_corpus, src_max_seq_len)\n",
    "    eval_en_corpus = padding(eval_en_corpus, src_max_seq_len)\n",
    "    train_ch_corpus = padding(train_ch_corpus, tgt_max_seq_len)\n",
    "    eval_ch_corpus = padding(eval_ch_corpus, tgt_max_seq_len)\n",
    "    \n",
    "    # truncate the vocabrary\n",
    "    # for the words exceeded te vocab size, we set it as 1(<UNK>)\n",
    "    train_en_corpus[train_en_corpus >= src_max_vocab_size] = 1\n",
    "    eval_en_corpus[eval_en_corpus >= src_max_vocab_size] = 1\n",
    "    train_ch_corpus[train_ch_corpus >= tgt_max_vocab_size] = 1\n",
    "    eval_ch_corpus[eval_ch_corpus >= tgt_max_vocab_size] = 1\n",
    "    \n",
    "#     train_en_corpus, eval_en_corpus, train_ch_corpus, eval_ch_corpus = train_test_split(train_en_corpus, train_ch_corpus, test_size=0.2, )\n",
    "\n",
    "    print('train size:{}, val size:{}'.format(train_en_corpus.shape, eval_en_corpus.shape))\n",
    "\n",
    "    iter_num = train_en_corpus.shape[0] // batch_size + 1  # 取整\n",
    "#     iter_num = 10\n",
    "\n",
    "    data_iterator = Iterator(train_en_corpus, train_ch_corpus)\n",
    "    eval_data_iterator = Iterator(eval_en_corpus, eval_ch_corpus)\n",
    "    now_lr = lr\n",
    "    with tf.Session(config=cf) as sess:\n",
    "        model = NMTModel(src_max_vocab_size=src_max_vocab_size, \n",
    "                         tgt_max_vocab_size=tgt_max_vocab_size, \n",
    "                         embedding_size=embedding_size,\n",
    "                         hidden_size=hidden_size,\n",
    "                         src_max_seq_len=src_max_seq_len,\n",
    "                         tgt_max_seq_len=tgt_max_seq_len,\n",
    "                         tgt_start_id=tgt_start_id,\n",
    "                         tgt_end_id=tgt_end_id,\n",
    "                         max_gradient_norm=max_gradient_norm,\n",
    "                         maximum_iterations=maximum_iterations,\n",
    "                         optimizer=optimizer)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        for epoch in range(num_epochs):\n",
    "            for iter_n in range(iter_num):\n",
    "                X, Y_out, Y_in, X_len, Y_in_len, Y_out_len = data_iterator.next_batch(batch_size)\n",
    "                # print(X.shape)\n",
    "                # print(Y_out.shape)\n",
    "                # print(Y_in.shape)\n",
    "                # print(X_len.shape)\n",
    "                # print(Y_in_len.shape)\n",
    "                # print(Y_out_len.shape)\n",
    "                loss, _, global_step = sess.run([model.loss_op, model.train_op, model.global_step], \n",
    "                    feed_dict={ model.X:X,\n",
    "                                model.Y_out:Y_out,\n",
    "                                model.Y_in:Y_in, \n",
    "                                model.X_len:X_len,\n",
    "                                model.Y_in_len:Y_in_len,\n",
    "                                model.Y_out_len:Y_out_len,\n",
    "                                model.lr:now_lr,\n",
    "                                model.dropout:dropout})\n",
    "                if iter_n % 100 == 0:\n",
    "                    print('iter:{}, train loss:{}'.format(iter_n, loss))\n",
    "            if optimizer == 'sgd':\n",
    "                now_lr = now_lr / 2\n",
    "            evaluate(model, sess, eval_data_iterator)\n",
    "            saver.save(sess,'model/my_model', global_step=global_step)\n",
    "\n",
    "\n",
    "def evaluate(model, sess, data_iterator):\n",
    "    translations = []\n",
    "    refs = []\n",
    "    losses = []\n",
    "    for X, Y_out, Y_in, X_len, Y_in_len, Y_out_len in data_iterator.next(batch_size):\n",
    "        loss, translation = sess.run([model.loss_op, model.translations], \n",
    "                        feed_dict={ model.X:X,\n",
    "                                    model.Y_in:Y_in,\n",
    "                                    model.Y_out:Y_out,\n",
    "                                    model.X_len:X_len,\n",
    "                                    model.Y_in_len:Y_in_len,\n",
    "                                    model.Y_out_len:Y_out_len,\n",
    "                                    model.lr:lr,\n",
    "                                    model.dropout:0.})\n",
    "#         print(translation)\n",
    "        print(translation.shape)\n",
    "        translations.append(translation)\n",
    "        refs.append(Y_out)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    translations = np.concatenate(translations, axis=0)  #？？？？\n",
    "    refs = np.concatenate(refs, axis=0)\n",
    "    bleu_score = bleu(refs, translations)\n",
    "    print('bleu score:{}, loss:{}'.format(bleu_score, np.mean(loss)))\n",
    "    \n",
    "#     # 写入结果\n",
    "#     translations1 = transform2word(translations, ch_id2word)\n",
    "#     with open(r'data/result.txt', 'w') as f:\n",
    "#         f.write(translations1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:(50000, 50), val size:(2500, 50)\n",
      "iter:0, train loss:340.5621032714844\n",
      "iter:100, train loss:274.78863525390625\n",
      "iter:200, train loss:222.52362060546875\n",
      "iter:300, train loss:252.895751953125\n",
      "iter:400, train loss:241.90365600585938\n",
      "iter:500, train loss:214.840576171875\n",
      "iter:600, train loss:194.01136779785156\n",
      "iter:700, train loss:238.8881072998047\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 37)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 40)\n",
      "(4, 40)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-f23c21c7cab3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-d42952e9d7cf>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sgd'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mnow_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnow_lr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m             \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_data_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m             \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'model/my_model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-d42952e9d7cf>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, sess, data_iterator)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m     \u001b[0mtranslations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#？？？？\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m     \u001b[0mrefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrefs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[0mbleu_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrefs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    with open('data/preprocess/vocab_dict.pkl', 'rb') as fr:\n",
    "        en_word2id, en_id2word, ch_word2id, ch_id2word = pkl.load(fr)\n",
    "    if type(X) == str:\n",
    "        X = X\n",
    "    elif type(x) == list or type(X) == tuple:\n",
    "        X = '\\n'.join(X)\n",
    "    else:\n",
    "        raise ValueError('You must ensure the `X` be string or list!')\n",
    "    X = segment(X, jieba.cut)\n",
    "    X = transform(X, en_word2id)\n",
    "    X = padding(X, src_max_seq_len)\n",
    "    X_len = np.sum((X > 0), axis=1)\n",
    "    # X -> (src_max_seq_len, ) or (batch, sec_max_seq_len, )\n",
    "    with tf.Session(config=cf) as sess:\n",
    "        model = NMTModel(src_max_vocab_size=src_max_vocab_size, \n",
    "                             tgt_max_vocab_size=tgt_max_vocab_size, \n",
    "                             embedding_size=embedding_size,\n",
    "                             hidden_size=hidden_size,\n",
    "                             src_max_seq_len=src_max_seq_len,\n",
    "                             tgt_max_seq_len=tgt_max_seq_len,\n",
    "                             tgt_start_id=tgt_start_id,\n",
    "                             tgt_end_id=tgt_end_id,\n",
    "                             max_gradient_norm=max_gradient_norm,\n",
    "                             maximum_iterations=maximum_iterations,\n",
    "                             optimizer=optimizer)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('model/'))\n",
    "        translations = sess.run(model.translations, \n",
    "                            feed_dict={ model.X:X,\n",
    "                                        model.Y_out:[[]],\n",
    "                                        model.Y_in:[[]], \n",
    "                                        model.X_len:X_len,\n",
    "                                        model.Y_in_len:[],\n",
    "                                        model.Y_out_len:[],\n",
    "                                        model.lr:lr,\n",
    "                                        model.dropout:0.})\n",
    "        translations = transform2word(translations, ch_id2word)\n",
    "    return translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/my_model-15640\n",
      "He is a friend of mine. --> 他是我的朋友。<PAD>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "src_sent = \"He is a friend of mine.\"\n",
    "pred = predict(src_sent)[0]\n",
    "print(src_sent + ' --> ' + pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/my_model-15640\n",
      "No doubt you would like to know how I have been getting along since I left school. --> 毫无疑问，你愿意知道我在离开学校的时候我已经走过了什么。<PAD>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "src_sent = \"No doubt you would like to know how I have been getting along since I left school.\"\n",
    "pred = predict(src_sent)[0]\n",
    "print(src_sent + ' --> ' + pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 3  4  5]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "b = np.array([[[1, 2, 3], [3, 4, 5]],[[7, 8, 9], [10, 11, 12]]])\n",
    "c = np.array([[7, 8, 9], [10, 11, 12]])\n",
    "# d = a.append(b)\n",
    "# print(d)\n",
    "c = np.concatenate(b, axis=0)\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
